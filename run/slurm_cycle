#!/usr/bin/env python
############################################################
##
## TODO, insert description
##
############################################################

## load buit-in python modules
import argparse
import os, shutil, sys
import subprocess as sp
from glob import glob
import datetime as dt

## load our own modules
import common
import slurm

## setup the logging system
log = common.setupLog()

##############################
## configurable
## TODO, make this configurable from outside
timeLimit_CFS = "7:00"
timeLimit_DA  = "7:00"



############################################################
## Get the Command line arguments

parser = argparse.ArgumentParser(description=(
    "CFS-LETKF 6 hour DA/forecast cycle script. Manages the SLURM "
    "submission of alternating LETKF and forecast jobs. This script "
    "monitors the job progress and relaunches any failed jobs. If "
    "a job fails due to a bad computational node, that node is flagged "
    "and will not be used again."))

## required variables
parser.add_argument('path', metavar="PATH", help=(
    "Path to the directory storing the experiment."))
parser.add_argument('start', metavar="START_DATE", help=(
    "Start date from which first forecast is launched. Date "
    "is in format YYYYMMDDHH"))
parser.add_argument('end', metavar="END_DATE", help=(
    "Stop date in format YYYYMMDDHH"))


## optional variables
parser.add_argument('--oobs', help=(
    "directory to ocean observations that are to be assimilated. "
    "If not specified, not ocean observations are used."))
parser.add_argument('--aobs', help=(
    "directory to atmospheric observations that are to be assimilated. "
    "If not specified, not atmospheric observations are used."))
parser.add_argument('--strong', action="store_true", default=False)

parser.add_argument('--ares', default='62', type=int, choices=common.aresList, help=(
    "Atmospheric resolution, must match the resolution of the initial "
    " ensemble members in the PATH directory. (Default: 62)"))
parser.add_argument('--clear', metavar="HOURS", default="12", type=int, help=(
    "Age (in hours) at which old no longer required individual ensemble "
    "members are deleted. Setting this to 0 disables this and leaves all "
    "members, and therefore uses a LOT of space. (Default: 12)"))
parser.add_argument('--mem', metavar='MEMBERS', type=int, help=(
    "The number of ensemble members. By default the script will "
    "automatically determine the number of ensemble members based "
    "on the directory structure of PATH"))

g = parser.add_argument_group("SLURM options")
g.add_argument('--account', default="aosc-hi", help=(
    "The SLURM account to run the experiment with. (Default: aosc-hi)"))
g.add_argument('--partition', help=(
    "The SLURM partition to use. Only meaningful overrides (on DT2) "
    " are 'debug' and 'scavenger'."))
g.add_argument('--retries', metavar="NUM", default="3", type=int, help=(
    "Maximum number of times to retry submitting a SLURM job step before "
    "This script fails out. (Default: 3)"))


## parse the variables 
args=parser.parse_args()
args.path = os.path.abspath(args.path)
if args.partition:
    slurm.partition = args.partition
args.start = dt.datetime.strptime(args.start, "%Y%m%d%H")
args.end   = dt.datetime.strptime(args.end, "%Y%m%d%H")
slurm.account = args.account
slurm.maxJobRetries = args.retries
#args.analGFS = os.getenv("CFSR_DIR")+"/T{}".format(args.ares)
if args.mem is None:
    args.mem = common.getEnsMem(args.path)
sdateShort = args.start.strftime("%Y%m%d%H")

############################################################
## print some basic information
common.addFileLog(log,args.path+'/logs/controller_'+sdateShort+'.log')
log.info("CFSv2-LETKF DA cycle script.")
log.info("Travis Sluka, University of Maryland, 2016")
log.info("")
log.info("Configuration:\n"+str(args)+"\n")


############################################################
## run the cycle
########################################
## number of cores for a CFS ensemble member run
nproc_cfs = int(os.getenv("NPROC_AM"))+int(os.getenv("NPROC_OM"))+1
nproc_letkf = int(os.getenv("NPROC_LETKF"))
nproc_pernode = int(os.getenv("NPROC_PERNODE"))
assert(nproc_cfs >= 3)
## number of nodes for a CFS ensemble member run
nnodes_cfs = nproc_cfs / nproc_pernode
nnodes_letkf = nproc_letkf / nproc_pernode

log.info("Using {:d} cores on {} nodes for each CFS ens member\n".format(
    nproc_cfs,nnodes_cfs))

cdate = args.start  ## current date
while cdate < args.end:
    log.info("*** Begining "+str(cdate))
    cdateShort = cdate.strftime("%Y%m%d%H")
    cdateDir = cdate.strftime("%Y/%Y%m/%Y%m%d/%Y%m%d%H")
    ndate = cdate + dt.timedelta(hours=6)     ## next date
    ndateShort = ndate.strftime("%Y%m%d%H")
    

    ##############################
    ## run the jobs that can be done completely parallel for each ens member.
    ## This consists of the forecast, conversion to gridded, and ocean obsop
    jobs = []
    for m in range(1, args.mem+1):
        mem = '{0:03d}'.format(m)
        cmd = "run_cycle_ensmem {} {} {} --ares {}".format(
            args.path, cdateShort, mem, args.ares)
        if args.oobs is not None: cmd += " --oobs "+args.oobs
        job = slurm.Job(
            name    = 'CFS_'+mem,
            cmd     = cmd,
            runtime = timeLimit_CFS,
            nproc   = nproc_cfs,
            nodes   = nnodes_cfs,
            output  = args.path+'/logs/'+cdateDir+'/CFS-'+mem+'_%j.log'
            )
        jobs.append(job)
    log.info("  Running parallel ens mem forecast and other jobs...")
    slurm.monitor(jobs)

    
    ##############################
    ## Run the observation operator for the atmosphere
    ##  This is done separately from the above step because the
    ##  superob program requires the mean of the atm ensemble
    cmd = "run_cycle_atmObsop {} {}".format(
        args.path, cdateShort)
    if args.aobs is not None: cmd += " --aobs "+args.aobs
    job = slurm.Job(
        name     = 'CFS_atmobs',
        cmd      = cmd,
        runtime  = "10:00",
        nproc    = nproc_pernode,
        output   = args.path+'/logs/'+cdateDir+'/atmObsop_%j.log'
        )
    log.info("  Running ATM observation operator...")    
    slurm.monitor([job,])


    ##############################
    ## Run the Data assimilation in parallel
    ##  for the two domains
    jobs = []
    cmd = "run_cycle_atmDA {} {} --ares {} --aobs".format(args.path, cdateShort, args.ares)
    if args.strong:  cmd += " --oobs"
    jobs.append(slurm.Job(
        name = "CFSDAAtm",
        cmd = cmd,
        runtime = timeLimit_DA,
        nproc = nproc_letkf,
        nodes   = nnodes_letkf,    
        output = args.path+'/logs/'+cdateDir+'/letkf-atm_%j.log'
    ))
    cmd = "run_cycle_ocnDA {} {} --oobs".format(args.path, cdateShort)
    if args.strong:  cmd += " --aobs"    
    jobs.append(slurm.Job(
        name = "CFSDAOcn",
        cmd = cmd,
        runtime = timeLimit_DA,
        nproc = nproc_letkf,
        nodes = nnodes_letkf,
        output = args.path+'/logs/'+cdateDir+'/letkf-ocn_%j.log'
    ))
    log.info("  Running LETKF...")
    slurm.monitor(jobs)
    

    
    ##############################
    ## clean up old files
    if args.clear > 0:
        files  = glob(args.path+'/gues/???/??????????_*')
        files += glob(args.path+'/anal/???/??????????.*')
        files += glob(args.path+'/obsop/??????????_*')
        for f in files:
            date = f.split('/')[-1].split('.')[0].split('_')[0]
            if (cdate - dt.datetime.strptime(date,"%Y%m%d%H")) >= dt.timedelta(hours = args.clear):
                os.remove(f)
                
    
    ## increment the date
    cdate = ndate
